{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Reinforcement learning for active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this experiment, we have implemented a custom OpenAI gym compatible environment to train a reinforcement learning agent to choose data points from available clusters to get optimal solution. The agent is trained using DQN algorithm implemented using Keras-rl as well as the custom implementation of DQN algorithm to compare the results. The model for learning is implemented using Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import all the required libraries\n",
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#import gym_activerl #Gym implementation for continuous action space.\n",
    "import gym_activerl1 #Gym implementation for discrete action space.\n",
    "#from dqn import DQNAgent ##Use this import to use the custom DQN implementation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam,SGD\n",
    "#Imports from Keras-rl Library\n",
    "from rl.agents.dqn import DQNAgent,NAFAgent \n",
    "from rl.policy import BoltzmannQPolicy,LinearAnnealedPolicy, EpsGreedyQPolicy,GreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('activerl1-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To debug and to test whether the environment is properly set up, one can run following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5488135 1.       ]\n",
      "(2,)\n",
      "835\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample()) #Samples an observation from observation space\n",
    "print(env.observation_space.shape)   #Displays the dimensions of the observation space\n",
    "print(env.action_space.sample())    #Samples an action from the action space\n",
    "print(env.action_space.n)           #Number of possible actions(For Discrete action space only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487456378339987\n",
      "(2,)\n",
      "[0.5 1. ]\n",
      "-1\n",
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_state = env.reset()                #Reset the environment \n",
    "print(env.theta)                           #Print the current theta of the environment(Which we need to predict)\n",
    "print(current_state)                       #Get current state of the \n",
    "cur,reward,episode_over,_ = env.step(0.5)  #Take an action and choose 0.5\n",
    "print(cur)                                 #Print the returned parameters.\n",
    "print(reward)\n",
    "print(episode_over)\n",
    "print(info)\n",
    "env.render()                               #visualize the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10375436216600131\n",
      "[0.705 1.   ]\n",
      "-2\n",
      "False\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cur,reward,episode_over,info = env.step(0.705)\n",
    "print(env.theta_n-env.theta)\n",
    "print(cur)\n",
    "print(reward)\n",
    "print(episode_over)\n",
    "print(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model which we use to decide the action to be taken. If we are using the custom DQN model, then the model should have env.action_space.n number of actions in the output layer. Else if we are using Keras-rl model then it should be 1 as there is only 1 discrete action possible in this environment. The output has sigmoid activation as we need the action to be between 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 15        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                120       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              21000     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 21,135\n",
      "Trainable params: 21,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))#\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1000))              #This layer has only 1 node when it is used by keras-rl agent. Else it should be equal to nb_actions(1000)\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=Adam(lr=0.001))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-RL implementation of DQN for discrete action space environment. The memory is the space allocated for the replay of RL agent. The policy used here is the Boltzmann Q policy. If one needs to use the Epsilon greedy Q policy just uncomment the statement. The weights of the trained RL agent is stored in h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -152.3830\n",
      "51 episodes - episode_reward: -29646.902 [-119070.000, 7.000] - loss: 17095.300 - mean_absolute_error: 0.383 - mean_q: 0.537 - Theta: 0.415 - Predicted Theta: 0.417\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -186.6723\n",
      "37 episodes - episode_reward: -50488.297 [-123278.000, -47.000] - loss: 24285.438 - mean_absolute_error: 0.203 - mean_q: 0.162 - Theta: 0.528 - Predicted Theta: 0.527\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -170.3443\n",
      "45 episodes - episode_reward: -38066.178 [-122282.000, 3.000] - loss: 24655.873 - mean_absolute_error: 0.179 - mean_q: 0.021 - Theta: 0.459 - Predicted Theta: 0.461\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -175.9688\n",
      "39 episodes - episode_reward: -45080.282 [-121410.000, -35.000] - loss: 26327.270 - mean_absolute_error: 0.184 - mean_q: 0.003 - Theta: 0.512 - Predicted Theta: 0.513\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -162.4397\n",
      "done, took 340.319 seconds\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2a2dca5306c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#dqn.load_weights('dqn_active_rl1_weights_1.h5f')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    356\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccumulated_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                             \u001b[0maccumulated_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0maccumulated_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "memory = SequentialMemory(limit=5000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,value_min=.1, value_test=.05,nb_steps=1000)\n",
    "dqn = DQNAgent(model=model,nb_actions=nb_actions, memory=memory,nb_steps_warmup=50,train_interval=1,gamma=0.95,\n",
    "               target_model_update=1, policy=policy)\n",
    "dqn.compile(Adam(lr=0.0001), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False,nb_max_episode_steps=500,nb_max_start_steps=0,start_step_policy=None\n",
    "        ,verbose=1)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_active_rl1_weights_1.h5f', overwrite=True)\n",
    "#dqn.load_weights('dqn_active_rl1_weights_1.h5f')\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom DQN agent implementation of DQN algorithm for discrete action space environment. It saves the weights when either the episode is over or the number of actions taken is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current trial 0\n",
      "Mean reward:  -238.876\n",
      "{'Theta': 0.4284709261868257, 'Predicted Theta': 0.42650002241134644}\n",
      "Current trial 1\n",
      "Mean reward:  -238.852\n",
      "{'Theta': 0.7400139207215027, 'Predicted Theta': 0.6800000071525574}\n",
      "Current trial 2\n",
      "Mean reward:  -243.744\n",
      "{'Theta': 0.563727084546804, 'Predicted Theta': 0.31850001215934753}\n",
      "Current trial 3\n",
      "Mean reward:  -247.556\n",
      "{'Theta': 0.7966156112496982, 'Predicted Theta': 0}\n",
      "Current trial 4\n",
      "Mean reward:  -246.864\n",
      "{'Theta': 0.46047009278625983, 'Predicted Theta': 0}\n",
      "Current trial 5\n",
      "Mean reward:  -245.744\n",
      "{'Theta': 0.7142069923697506, 'Predicted Theta': 0}\n",
      "Current trial 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3505f778e879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# internally iterates default (prediction) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# iterates target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mreward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML Research/dqn.py\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#print(target.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mQ_future\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials  = 50000\n",
    "trial_len = 500\n",
    "\n",
    "dqn_agent = DQNAgent(env=env,model=model)\n",
    "steps = []\n",
    "\n",
    "for trial in range(int(trials/trial_len)):\n",
    "    print(\"Current trial {}\".format(trial))\n",
    "    cur_state = env.reset().reshape(1,2)\n",
    "    dqn_agent.forget()\n",
    "    reward_list = list()\n",
    "    for step in range(trial_len):\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state = new_state.reshape(1,2)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "        dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "        dqn_agent.target_train() # iterates target model\n",
    "        reward_list.append(reward)\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            break\n",
    "    if step >= trial_len:\n",
    "        print(\"Failed to complete in trial {}\".format(trial))\n",
    "        print(\"Mean reward: \",np.mean(reward_list))\n",
    "        print(info)\n",
    "        if step % 10 == 0:\n",
    "            dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "    elif done:\n",
    "        print(\"Mean reward: \",np.mean(reward_list))\n",
    "        print(info)\n",
    "        print(\"Completed in {} trials\".format(trial))\n",
    "        dqn_agent.save_model(\"success.model\")\n",
    "    else:\n",
    "        print(\"Mean reward: \",np.mean(reward_list))\n",
    "        print(info)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
